# -*- coding: utf-8 -*-
import re, codecs
import os
import unicodedata
import string
from pyvi import ViTokenizer

REGEX_SPECIAL = '*^$*+?!#|\\()[]'

cur_dir = os.path.abspath(__file__)
cur_dir = os.path.dirname(cur_dir)

accents = f'{cur_dir}/map/accents.txt'
syllables = f'{cur_dir}/map/syllables.txt'
abbrev = f'{cur_dir}/map/abbreviations.txt'
url = f'{cur_dir}/map/url.txt'

with codecs.open(accents, 'r', encoding='UTF-8') as f:
  accents_dict = {}
  for line in f.readlines():
    k, v = line.split('\t')
    accents_dict[k] = v.replace('\n', '')

with codecs.open(syllables, 'r', encoding='UTF-8') as f:
  syllables_dict = {}
  for line in f.readlines():
    k, v = line.split('\t')
    syllables_dict[k] = v.replace('\n', '')

with codecs.open(abbrev, 'r', encoding='UTF-8') as f:
  abbrev_dict = {}
  for line in f.readlines():
    k, v = line.split('\t')
    abbrev_dict[k] = v.replace('\n', '')

with codecs.open(url, 'r', encoding='UTF-8') as f:
  re_urls = []
  for url in f.readlines():
    re_urls += [re.compile(url.strip('\n'))]

def remove_html(text):
  return re.sub(r'<[^>]*>', '', text)
 
def convert_utf8(text):
  return unicodedata.normalize('NFC', text)


def normalize_accent(text):
  """a', o` -> √°, √≤"""
  for k, v in accents_dict.items():
    text = text.replace(k, v)  
  return text 

def normalize_syllable(text):
  """√≤a, √≤e, √∫y -> o√†, o√®, u√Ω"""
  for k, v in syllables_dict.items():
    text = text.replace(k, v)  
  return text 

def normalize_abbrev(text):
  for k, v in abbrev_dict.items():
    #text = text.replace(k, v)
    k = ''.join('\\'+c if REGEX_SPECIAL.find(c) > -1 else c for c in k)
    r = re.compile(fr'\b{k}\b', flags=re.IGNORECASE)
    text = r.sub(v, text)
  return text

def normalize_repetive(text):
  return re.sub(r'([A-Z])\1+', lambda match: match[1], text, flags=re.IGNORECASE)

def remove_url(text):
  for re_url in re_urls:
    text = re_url.sub('', text) 
  return text

def normalize(text):
  text = remove_html(text)
  text = remove_url(text)
  text = convert_utf8(text)
  text = normalize_accent(text)
  text = normalize_abbrev(text)
  text = normalize_syllable(text)
  text = normalize_repetive(text)

  # Normalize some well-known emojis
  text = re.sub(r'[:=]([\)\]}])\1+', 'üôÇ', text)
  text = re.sub(r'[:=]([\(\[{])\1+', 'üôÅ', text)
  text = re.sub(r'[:=]([vV])+', 'ü§£', text)
  
  # Remove punctuation
  punc = ''.join([p for p in string.punctuation if p not in '.!?,'])
  translator = str.maketrans(punc, ' '*len(punc))
  text = text.translate(translator)
  text = text.translate(str.maketrans('.!?', '.'*3))

  # Word segmentation
  text = ViTokenizer.tokenize(text, )
  
  return text


# Testing
texts = [
  "ko",
  "t/k/c·∫ßn bik b·∫°n l√† ai. B·∫°n kh√¥ng n·ªï ƒë·ªãa ch·ªâ t·∫°i abc@gmail.com hay google.com th√¨ ch·∫øt cm m√†y v·ªõi t! Cho m√†y 2 ng√†y",
  "Nh·∫•t c√¥ H·∫±ng lun, 5* c√¥ ∆°i :))",
  "v·∫Ω h·ªça ti·∫øt",
  "ca^u √¢'m co^ chie^u", 
  "   la'o    nga    la'o ng√°o. hoa' ƒëi√™n ho'a kh√πng",
  r":v :vvv :VVV :))) :}}}} :) :((( ={{{",
  "Ch·∫Øc ƒëc ƒë√≥. H√¥m qua tao c√≥ n√≥i v·ªõi n√≥ r·ªìi m√†",
  "ngoooonnnnn",
  '=]]]]]]',
  '<p class=h> abc </p>'
]
decomposed_texts = [unicodedata.normalize('NFD', t) for t in texts]

normalized_texts = [normalize(t) for t in texts]
normalized_texts_decomposed = [normalize(t) for t in decomposed_texts]

for t in normalized_texts:
  print(t)
print(texts == decomposed_texts)
print(normalized_texts == normalized_texts_decomposed)
